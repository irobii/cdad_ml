{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "회사에서 제품 런칭이나 중요 이벤트에 대한 공공 반응을 살피는 것이 필수.\n",
    "\n",
    "Opinion mining: twitter에서는 사용자가 생산하는 내용물에 실시간으로 쉽게 접근할 수 있어 tweet  의 감성 분류가 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter 분석시 사전 셋팅\n",
    "\n",
    "1. Install twitter\n",
    "```\n",
    "$ pip3 install twitter\n",
    "Collecting twitter\n",
    "  Downloading https://files.pythonhosted.org/packages/85/e2/f602e3f584503f03e0389491b251464f8ecfe2596ac86e6b9068fe7419d3/twitter-1.18.0-py2.py3-none-any.whl (54kB)\n",
    "    100% |████████████████████████████████| 61kB 380kB/s \n",
    "Installing collected packages: twitter\n",
    "Successfully installed twitter-1.18.0\n",
    "```\n",
    "\n",
    "2. https://apps.twitter.com/app/new에서 twitter app을 등록하고 토큰 설정 (https://m.blog.naver.com/acwboy/220541273950 참조) \n",
    "<br> -> 등록 절차가 복잡하고 어려움. e-mail로 재 작성 요청받았으나 결국 twitter rule violation으로 fail\n",
    "\n",
    "\n",
    "3. Personal twitter access keys and tokens를 받아서 twitterauth.py에 keys/secrets를 붙여 넣는다. (Twitter API 1.0부터 적용)\n",
    "\n",
    "4. Building Machine Learning Systems With Python의 저자 github [chapter6](https://github.com/luispedro/BuildingMachineLearningSystemsWithPython/tree/master/ch06) 에 있는 install.py 실행\n",
    "\n",
    "```\n",
    "$ python install.py \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 지식\n",
    "\n",
    "### Bayesian theorum\n",
    "\n",
    "[세상에서 가장 쉬운 베이즈통계학 입문 1강,2강](https://github.com/biospin/cloudnomad_bio/blob/master/part01/week1_0905/%EC%84%B8%EC%83%81%EC%97%90%EC%84%B8%EA%B0%80%EC%9E%A5%EC%89%AC%EC%9A%B4%EB%B2%A0%EC%9D%B4%EC%A6%88%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8_1%EA%B0%952%EA%B0%95.pdf)\n",
    "\n",
    "### N grams, etc..\n",
    "\n",
    "[N-grams](https://docs.google.com/document/d/10kW-3nlizpVtiS3nNmo_8syMwlgJpdNOfGgVunE1hQ8/edit)\n",
    "\n",
    "[TF_IDF](https://ko.wikipedia.org/wiki/Tf-idf) [TF IDF (영문)](https://en.wikipedia.org/wiki/Tf–idf)\n",
    "\n",
    "### Smooting\n",
    "\n",
    "[Additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)\n",
    "\n",
    "### scikit learn 매뉴얼\n",
    "\n",
    "[naive_bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "[naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "[naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "[tips](https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44?gi=810449792807)\n",
    "\n",
    "\n",
    "## 참고자료\n",
    "\n",
    "[Online equation editor](https://www.codecogs.com/latex/eqneditor.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  큰 그림 그리기\n",
    "\n",
    "트위터의 문자 수가 140자로 제한되어 감성 분석이 어려움. (특수한 문법, 약어, 잘 형식화되지 않은 문장이 많음)\n",
    "\n",
    "Naive Bayes를 분류 알고리즘으로 소개하는 도구로 사용함.\n",
    "\n",
    "품사 태깅이 어떻게 작동하고 도움이 되는지 설명.\n",
    "\n",
    "Scikit-learn 툴박스에 기능 보여줌,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터 데이터 가져오기\n",
    "\n",
    "트윗이 부정, 긍정, 중립적인 감정을 포함하는 라벨이 필요.\n",
    "\n",
    "5,000개 이상의 트윗에 수작업으로 라벨을 붙이고 사용을 허락한 Nick Sanders의 트윗 ID와 수작업 라벨 감성이 포함된 말뭉치를 \n",
    "install.py을 사용해 가져옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "X, Y = load_sanders_data()\n",
    "\n",
    "classes = np.unique(Y)\n",
    "\n",
    "for c in classes:\n",
    "    print(\"#%s: %i\" % (c, sum(Y==c)))\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이즈 분류기 소개\n",
    "\n",
    "명쾌한 기계 학습 알고리즘이지만, 분류 성능은 naive하지 않음.\n",
    "\n",
    "상관 없는 속성에 꽤 robust하다고 증명됨 => 빠르게 학습하고 배운대로 예측하며 많은 저장 공간도 필요 없음.\n",
    "\n",
    "'naive'는 베이즈가 최적으로 작동하는 데 필요한 가정 (=모든 속성은 서로 독립적)을 설명하고자 붙여짐. 이 가정은 실세계 애플리케이션에서 드문 경우지만, 독립 가정이 유효하지 않을때도 괜찮은 정확도를 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이즈 정리\n",
    "\n",
    "나이브 베이즈 분류의 핵심은, 분류의 evidence를 고려해 속성을 파악하는 것.\n",
    "\n",
    "변수 | 가능한 값 | 의미 \n",
    "----| ---------|-----------------------\n",
    " C | pos, neg |  트윗의 범주 (긍정/부정)\n",
    " $F_1$ | 음이 아닌 정수 | 트윗에서 'awesome'의 빈도수\n",
    " $F_2$ | 음이 아닌 정수 | 트윗에서 'crazy'의 빈도수\n",
    " \n",
    " 훈련하는 동안, 속성  $F_1$과  $F_2$를 알고 있을 때 범주 C에 대한 확률 ($P(C| F_1, F_2)$) 인 나이브 베이즈 모델이 만들어짐.\n",
    " \n",
    " $ P(A) \\cdot P(B|A) = P(B)  \\cdot P(A|B)$\n",
    " \n",
    " A를 두 속성 $F_1$과  $F_2$의 빈도의 확률로 대체하고 B를 범주 C로 대체하면, 특정한 범주에 속하는 데이터 인스턴스의 확률을 검색할 수 있게 도움을 주는 관계식이 구해짐.\n",
    " \n",
    " $P(C|F_1, F_2) = \\frac{P(C) \\cdot P(F_1, F_2 | C)}{P(F_1, F_2)}$은 다음 개념으로 나타낼 수 있음.\n",
    " \n",
    " $posterior = \\frac{prior \\cdot likelihood}{evidence}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브 베이즈를 사용한 분류\n",
    "\n",
    "새로운 트윗을 고려해, 남은 부분의 확률을 계산해야 한다.\n",
    "\n",
    "$P(C = \"pos\"|F_1, F_2) = \\frac{P(C=\"pos\") \\cdot P(F_1 | C = \"pos\") \\cdot P(F_2 | C = \"pos\")}{P(F_1, F_2)}$\n",
    "\n",
    "$P(C = \"neg\"|F_1, F_2) = \\frac{P(C=\"neg\") \\cdot P(F_1 | C = \"neg\") \\cdot P(F_2 | C = \"neg\")}{P(F_1, F_2)}$\n",
    "\n",
    "좀 더 높은 확률을 갖는 $C_{best}$ 범주를 선택할 필요가 있음. 두 범주 모두 분모가 같기 때문에 이를 무시할 수 있음.\n",
    "\n",
    "실제 확룰이 아닌 범주의 가능성이 있는 정보에 관심이 있음. 이를 다음 수식으로 나타낼 수 있음.\n",
    "\n",
    "$$C_{best}=argmax_{c \\subseteq C}P(C=c) \\cdot P(F_1 | C = c) \\cdot P(F_2 | C = c)$$\n",
    "\n",
    "여기서 모든 C의 범주 (즉, \"pos\"와 \"neg\")에 대해 'arg max' 뒷부분을 계산하고 최대값으로 결과로 나온 범주를 반환.\n",
    "\n",
    "다음 예제에서 실제 확률을 이용해 나이브 베이즈가 어떻게 작동되는지 계산해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet**       |  **Class**\n",
    " ----------| ---------\n",
    " awesome | Positive tweet\n",
    " awesome | Positive tweet\n",
    " awesome crazy | Positive tweet\n",
    " crazy | Positive tweet\n",
    " crazy | Negative tweet\n",
    " crazy | Negative tweet\n",
    " \n",
    " 총 6개 트윗 중 4개는 긍정, 2개는 부정적. 다음과 같은 사전 확률을 알 수 있음.\n",
    " \n",
    " $P(C = \"pos\") =\\frac{4}{6} \\approx 0.67$\n",
    " \n",
    " $P(C = \"neg\") =\\frac{2}{6} \\approx 0.33$\n",
    " \n",
    " 이를 통해, 트윗에 대한 아무 정보가 없을 때 '트윗은 긍정적이다'라고 가정할 수 있음.\n",
    " \n",
    " 속성 $F_1$, $F_2$일때 확률 계산을 해보자.\n",
    " \n",
    "$P(F_1 =1 | C = \"pos\") = \\frac{'awesome'이 한번이라도 포함된 pos 트윗의 수}{pos 트윗의 수} = \\frac{3}{4} = 0.75$\n",
    "\n",
    "$P(F_1 =0 | C = \"pos\") = 1- P(F_1 =1 | C = \"pos\") = 0.25$\n",
    "\n",
    "$P(F_2 =1 | C = \"pos\") = \\frac{2}{4} = 0.5$\n",
    "\n",
    "$P(F_1 =1 | C = \"neg\") = \\frac{0}{2} = 0$\n",
    "\n",
    "$P(F_2 =1 | C = \"neg\") = \\frac{2}{2} = 1$\n",
    "\n",
    "$P(F_1 , F_2) = {P(F_1 , F_2 | C = \"pos\") \\cdot P(C = \"pos\")} + P(F_1 , F_2 | C = \"neg\") \\cdot P(C = \"neg\")$\n",
    "\n",
    "실제 값을 넣으면 다음과 같음.\n",
    "\n",
    "$P(F_1=1 , F_2=1) = \\frac{1}{3} \\cdot \\frac{4}{6} + 0 \\cdot \\frac{2}{6} = 0.22$\n",
    "\n",
    "$P(F_1=1 , F_2=0) = \\frac{2}{3} \\cdot \\frac{4}{6} + 0 \\cdot \\frac{2}{6} = 0.44$\n",
    "\n",
    "$P(F_1=0 , F_2=1) = 0 \\cdot \\frac{4}{6} + 1 \\cdot \\frac{2}{6} = 0.33$\n",
    "\n",
    "$P(F_1=0 , F_2=0) = 0$\n",
    "\n",
    "이제 트윗을 분석하고 속성을 부여하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet**      |  **F1**     |  **F2**      |  **Class probabilities**   |  **Classification**\n",
    " ----------     | --------- | --------- | ------------------------- | ------------------\n",
    "\"awesome\"     |         1      |        0        |   $$P(C = \"pos\"|F_1=1, F_2=0) =\\frac{\\frac{3}{4}\\cdot\\frac{2}{4}\\cdot\\frac{4}{6}}{\\frac{1}{4}}=1$$ $$P(C = \"neg\"|F_1=1, F_2=0) =\\frac{\\frac{0}{2}\\cdot\\frac{2}{2}\\cdot\\frac{2}{6}}{\\frac{1}{4}}=0$$          |         Positive\n",
    "\"crazy\"   |         0     |        1        |   $$P(C = \"pos\"|F_1=0, F_2=1) =\\frac{\\frac{1}{4}\\cdot\\frac{2}{4}\\cdot\\frac{4}{6}}{\\frac{5}{12}}={\\frac{1}{5}}$$ $$P(C = \"neg\"|F_1=0, F_2=1) =\\frac{\\frac{2}{2}\\cdot\\frac{2}{2}\\cdot\\frac{2}{6}}{\\frac{5}{12}}={\\frac{4}{5}}$$   |         Negative\n",
    "\"awesome crazy\"   |         1      |        1        |    $$P(C = \"pos\"|F_1=1, F_2=1) =\\frac{\\frac{3}{4}\\cdot\\frac{2}{4}\\cdot\\frac{4}{6}}{\\frac{1}{4}}=1$$ $$P(C = \"neg\"|F_1=1, F_2=1) =\\frac{\\frac{0}{2}\\cdot\\frac{2}{2}\\cdot\\frac{2}{6}}{\\frac{1}{4}}=0$$   |         Positive\n",
    "text  |         0      |        0        |    $$P(C = \"pos\"|F_1=1, F_2=1) =\\frac{\\frac{0.67\\cdot0.75\\cdot0}{0}}=?$$ $$P(C = \"neg\"|F_1=1, F_2=1) =\\frac{\\frac{0.33\\cdot0\\cdot0}{0}}=?$$   |         미지정 (이전에 트윗에서 이러한 단어를 본 적이 없음)\n",
    "\n",
    "마지막 경우는 어떻게 처리할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 못 보던 단에어 대한 해명과 기타 특이점\n",
    "\n",
    "이전 확률을 계산할때 실제 확률을 계산하지 않고 일부 대략적인 근사치를 계산했음. 6개의 트윗 corpus (말뭉치)로 모든 트윗에 대한 정보를 명확하게 할 수 없음. 'text'라는 이전에 본 적이 없는 단어를 설명하기 위해 '1 더하기 평활화 add-one smoothing' 를 이용해 해결한다.\n",
    "\n",
    "\"Add-one smoothing is sometimes also referred to as additive smoothing or Laplace smoothing. Note that Laplace smoothing has nothing to do with Laplacian smoothing, which is related to the smoothing of polygon meshes. If we do not smooth by 1 but by an adjustable parameter alpha<0, it is called Lidstone smoothing.\"\n",
    "\n",
    "add-one smoothing은 모든 카운트마다 1을 더하는 기술. 전체 corpus에 주어진 단어가 없더라도 그 단어를 포함하는 트윗 샘플이 있는 경우도 존재한다고 가졍하여 한 번 이상 발생한 것으로 처리.\n",
    "\n",
    "$P(F_1 =1 | C = \"pos\") = \\frac{3}{4} = 0.75$\n",
    "대신 다음과 같이 새롭게 계산한다.\n",
    "\n",
    "$P(F_1 =1 | C = \"pos\") = \\frac{3+1}{4+2} = 0.67$\n",
    "\n",
    "분모에 2를 더하는 이유는 모든 확률 값을 더해 1로 만들기 위해 정규화 하기 위함. 현재 데이터셋 'awesome'은 한 번도 발생하지 않거나 한번은 발생해야 하는 두 가지 경우이기 때문에 총 확률은 1.\n",
    "\n",
    "$P(F_1 =1 | C = \"pos\") + P(F_1 =0 | C = \"pos\") = \\frac{3+1}{4+2} + \\frac{1+1}{4+2} = 1 $\n",
    "\n",
    "같은 방법으로 , posterior도 구한다.\n",
    "\n",
    "$P(C = \"pos\") = \\frac{4+1}{6+2}  \\approx 0.625$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arithmetic underflows (산술 언더플로)\n",
    "\n",
    "또 다른 문제는 실제로는 지금까지 다룬 예제보다 훨씬 작은 확률로 작업한다는 점. 현실적으로는 서로 곱한 2개 이상의 속성을 가짐. Numpy가 더 이상 충분히 지원할 수 없는 정확도를 요구."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=20) # tell numpy to print out more digits (default is 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.e-324])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2.48E-324])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2.47E-324])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.47E-324은 도대체 어떤 값인가? 이에 답하기 위해 0.0001 조건 확률 가능성을 상상하고 거기에 65 번을 서로 곱함 (meaning that we have 65 low probable feature values) and you've been hit by the arithmetic under ow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.00001\n",
    "x**64 # still fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**65 # ouch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬의 float는 C 언어의 double 형으로 구현하였으면 다음과 같이 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.float_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpmath (http://code. google.com/p/mpmath/) 로 변경하여 해결 할 수 있으나 NumPy를 대체할 만큼 빠르지 않음.\n",
    "\n",
    "log 값을 취하여 곱을 합으로 변환함으로써 해결할 수 있음.\n",
    "\n",
    "$log(P(C)\\cdot log(F_1|C)\\cdot P(F_2|C)) = log(P(C)) + log(P(F_1|C)) + log(P(F_2|C))$\n",
    "\n",
    "확률이 0과 1 사이 구간에 있으므로, 확률의 로그 값은 -무한대와 0 사이에 놓임.\n",
    "\n",
    "(그림: 확률 값과 확률 값에 대한 로그 값의 관계 p159)\n",
    "\n",
    "임의의 속성 개수로 다시 식을 써보자.\n",
    "\n",
    "$C_{best} = arg max_{c \\subseteq C}log(P(C=c)) + \\sum_klog(P(C_k|C=c))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 번째 분류기 만들고 조정하기\n",
    "\n",
    "sklearn.naive_bayes 패키지의 나이브 베이즈 종류 (sklearn 공식 매뉴얼 참고)\n",
    "\n",
    "- GaussianNB: 속성이 일반적인 가우시안 분포를 따른다고 가정. 특정한 사람의 키와 너비에 따라 성별을 구별하는 경우에 사용. 단어 수를 추출할 트윗 텍스트는 가우시안 분포를 따르지 않아 적절하지 않음.\n",
    "\n",
    "- MultinomialNB: 속성이 일어나는 빈도를 가정. 투윗의 단어 빈도를 속성으로 사용하기 때문에 적합. TF-IDF 벡터와 잘 작동함.\n",
    "\n",
    "- BernoulliNB: MultinomialNB와 유사하나 binary 에 더 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쉬운 문제 먼저 해결하기\n",
    "\n",
    "트윗에 대부분 중립적인 정보를 담고 있으나, 복잡한 작업을 피하기 위해 긍정적이거나 부정적인 트윗만을 우선 중심으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> # first create a Boolean list having true for tweets that are either positive or negative\n",
    ">>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n",
    ">>> # now use that index to filter the data and the labels\n",
    ">>> X = X[pos_neg_idx]\n",
    ">>> Y = Y[pos_neg_idx]\n",
    ">>> # finally convert the labels themselves into Boolean\n",
    ">>> Y = Y==\"positive\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X는 트윗 텍스트 원본이고, Y는 이진 분류. 0을 부정적, 1을 긍정적인 트윗에 지정. 트윗 텍스트를 TF-IDF 속성으로 변환하도록 TfidfVectorizer를 만든다 (3장 참고).\n",
    "\n",
    "첫 번째 분류기를 훈련시키기 위해 TF-IDF 속성과 라벨을 함께 사용. 편의상 백터라이저와 분류기를 함께 연결하도록 만들어주고 같은 인터페이스를 제공하는 Pipeline 클래스를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_ngram_model():\n",
    "    tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   analyzer=\"word\", binary=False)\n",
    "    clf = MultinomialNB()\n",
    "    return Pipeline([('vect', tfidf_ngrams), ('clf', clf)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_ngram_model () 은 일반 분류기처럼 fit()와 predict()를 사용할 수 있는 Pipeline 인스턴스를 반환한다.\n",
    "\n",
    "데이터가 많지 않아 교차 검증을 사용하고 각 fold에서 중복되지 않도록 데이터를 섞어주는 ShuffleSplit을 이용. 각 fold에서 precision-recall curve와 accuracy를 기록한다.\n",
    "\n",
    "agile한 실험을 유지하기 위해, 분류기를 매개변수로 갖는 train_model() 함수에 모든 것을 넣기로 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "def train_model(clf_factory, X, Y):\n",
    "    # setting random_state to get deterministic behavior\n",
    "    cv = ShuffleSplit(n=len(X), n_iter=10, test_size=0.3, random_state=0)\n",
    "    \n",
    "    scores = []\n",
    "    pr_scores = []\n",
    "    \n",
    "    for train, test in cv:\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], Y[test]\n",
    "        \n",
    "        clf = clf_factory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        scores.append(test_score)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        \n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])\n",
    "        \n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        \n",
    "        summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
    "        print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % summary)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> X, Y = load_sanders_data()\n",
    ">>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n",
    ">>> X = X[pos_neg_idx]\n",
    ">>> Y = Y[pos_neg_idx]\n",
    ">>> Y = Y==\"positive\"\n",
    ">>> train_model(create_ngram_model, X, Y)\n",
    "0.805   0.024   0.878   0.016\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터화한 TF-IDF 트라이그램 속성에 대해 나이브 베이즈를 사용한 시도는 80.5%의 accuracy와 87.8%의 P/R AUC 값을 얻는다. 다음 그림의 P/R 차트를 보면 이전 장에서 봤던 도표보다 훨씬 더 고무적임. (p163 그림 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 범주 사용\n",
    "\n",
    "트윗에 감정 요소가 포함되어 있는지 여부를 분류하기 위해 어떻게 수행해야 하는가? 이를 위해 감정 요소가 있다면 양성으로 감성 트윗의 범주를 반환하는 함수를 작성하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def tweak_labels(Y, pos_sent_list):\n",
    "    pos = Y==pos_sent_list[0]\n",
    "    for sent_label in pos_sent_list[1:]:\n",
    "        pos |= Y==sent_label\n",
    "    Y = np.zeros(Y.shape[0])\n",
    "    Y[pos] = 1\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "return Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감성 트윗은 훈련 데이터의 범주와 구별되는 양성이 된다. 예를 들어, 중립적인 트윗과 감성 트윗을 잘 구별할 수 있는 방법을 찾기 위해 다음과 같이 할 수 있다.\n",
    "```\n",
    ">>> Y = tweak_labels(Y, [\"positive\", \"negative\"])\n",
    "```\n",
    "Y에서 긍적적이거나 부정적인 트윗은 1, 중립적이거나 상관없는 트윗은 0을 갖는다.\n",
    "```\n",
    ">>> train_model(create_ngram_model, X, Y, plot=True)\n",
    "0.767   0.014   0.670   0.022\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도는 높지만 데이터셋이 불균형하기 때문 P/R AUC는 67%로 상당히 떨어짐.\n",
    "\n",
    "긍정적인 트윗과 나머지 트윗, 부정적인 트윗과 나머지 트윗으로 분류해도 성능이 좋지 않음. (p165 그림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류기의 매개변수 조절\n",
    "\n",
    "- TfidVectorizer\n",
    "\n",
    "> N gram: unigram (1,1), bigram (1,2), trigram (1,3)\n",
    "\n",
    "> min_df: 1 / 2\n",
    "\n",
    "> use_idf, smooth_idf: False / True\n",
    "\n",
    "> stop word 제거 또는 english나 None으로 설정 변경\n",
    "\n",
    "> sublinear_tf: 단어의 빈도에 대해 로그를 사용할지 여부\n",
    "\n",
    "> 단어 카운트를 추적 할지 여부, 단어빈도로 할지, binary 설정(True / False)\n",
    "\n",
    "- MultinomialNB\n",
    "\n",
    "> alpha 값을 설정해 사용하기 위해 다음 smoothing 방법 결정\n",
    "\n",
    ">° Add-one or Laplace smoothing: 1\n",
    "\n",
    "> ° Lidstone smoothing: 0.01, 0.05, 0.1, or 0.5\n",
    "\n",
    "> ° No smoothing: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn에서 제공한는 GridSearchCV 클래스 이용해 best_estimator_ 멤버 변수로 최적의 매개변수를 설정\n",
    "\n",
    "sklearn.metrics 패키지의 평가 함수를 사용할 수 있음.\n",
    "\n",
    "matrics.f1_score $F = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "def grid_search_model(clf_factory, X, Y):\n",
    "    cv = ShuffleSplit(\n",
    "     n=len(X), n_iter=10, test_size=0.3,random_state=0)\n",
    "    param_grid = dict(vect__ngram_range=[(1, 1), (1, 2), (1, 3)],\n",
    "        vect__min_df=[1, 2],\n",
    "        vect__stop_words=[None, \"english\"],\n",
    "        vect__smooth_idf=[False, True],\n",
    "        vect__use_idf=[False, True],\n",
    "        vect__sublinear_tf=[False, True],\n",
    "        vect__binary=[False, True],\n",
    "        clf__alpha=[0, 0.01, 0.05, 0.1, 0.5, 1],\n",
    "        )\n",
    "    grid_search = GridSearchCV(clf_factory(),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        score_func=f1_score,\n",
    "        verbose=10)\n",
    "    grid_search.fit(X, Y)\n",
    "    return grid_search.best_estimator_\n",
    "```    \n",
    "다음 코드를 실행하고 기다린다.\n",
    "\n",
    "```\n",
    "clf = grid_search_model(create_ngram_model, X, Y)\n",
    "print(clf)\n",
    "```\n",
    "\n",
    "매개변수 조합 $3 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 6 = 1152$ 개만큼 변경하려면 매개변수를 요청하며, 각각을 10 fold에 대해 훈련을 해야 하기 때문에 몇 시간을 기다려야 한다.\n",
    "\n",
    "```\n",
    "Pipeline(clf=MultinomialNB(\n",
    "alpha=0.01, class_weight=None, fit_prior=True),\n",
    "clf__alpha=0.01,\n",
    "clf__class_weight=None,\n",
    "clf__fit_prior=True,\n",
    "vect=TfidfVectorizer(\n",
    "analyzer=word, binary=False,\n",
    "   charset=utf-8, charset_error=strict,\n",
    "dtype=<type 'long'>,input=content,\n",
    "lowercase=True, max_df=1.0,\n",
    "max_features=None, max_n=None,\n",
    "min_df=1, min_n=None, ngram_range=(1, 2),\n",
    "norm=l2, preprocessor=None, smooth_idf=False,\n",
    "stop_words=None,strip_accents=None,\n",
    "sublinear_tf=True,token_pattern=(?u)\\b\\w\\w+\\b,\n",
    "token_processor=None, tokenizer=None,\n",
    "use_idf=False, vocabulary=None),\n",
    "vect__analyzer=word, vect__binary=False,\n",
    "vect__charset=utf-8,\n",
    "vect__charset_error=strict,\n",
    "vect__dtype=<type 'long'>,\n",
    "vect__input=content, vect__lowercase=True,\n",
    "vect__max_df=1.0,vect__max_features=None,\n",
    "vect__max_n=None, vect__min_df=1,\n",
    "vect__min_n=None, vect__ngram_range=(1, 2),\n",
    "vect__norm=l2, vect__preprocessor=None,\n",
    "vect__smooth_idf=False, vect__stop_words=None,\n",
    "vect__strip_accents=None, vect__sublinear_tf=True,\n",
    "vect__token_pattern=(?u)\\b\\w\\w+\\b,\n",
    "vect__token_processor=None, vect__tokenizer=None,\n",
    "vect__use_idf=False, vect__vocabulary=None)\n",
    "0.795  0.007  0.702  0.028\n",
    "```\n",
    "\n",
    "The best estimator indeed improves the P/R AUC by nearly 3.3 percent to now 70.2, with the settings shown in the previous code.\n",
    "Also, the devastating results for positive tweets against the rest and negative tweets against the rest improve if we con gure the vectorizer and classi er with those parameters we have just found out:\n",
    "```\n",
    " == Pos vs. rest ==\n",
    "0.889   0.010   0.509\n",
    "== Neg vs. rest ==\n",
    "0.886   0.007   0.615\n",
    "0.041 0.035\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트윗 정리\n",
    "\n",
    "감정 이모디콘과 단축어를 고려해 TfidVectorizer의 preprocessor()를 구현. 긍정적이거나 부정적인 단어로 대체할 수 ㅇㅆ는 이모디콘을 찾아 빈도 범위와 딕셔너리의 대체 단어를 정의하여 사용하면 분류기에 도움이 됨.\n",
    "\n",
    "```\n",
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \", # :D in lower case\n",
    "    \":dd\": \" good \", # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "    }\n",
    "# make sure that e.g. :dd is replaced before :d\n",
    "emo_repl_order = [k for (k_len,k) in reversed(sorted([(len(k),k) for k in\n",
    "emo_repl.keys()]))]\n",
    "```\n",
    "\n",
    "그런 다음, 약어의 확장어와 함께 정규 표현식으로 정의 (\\b 는 word boundary를 나타냄):\n",
    "\n",
    "```\n",
    "re_repl = {\n",
    "r\"\\br\\b\": \"are\",\n",
    "r\"\\bu\\b\": \"you\",\n",
    "r\"\\bhaha\\b\": \"ha\",\n",
    "r\"\\bhahaha\\b\": \"ha\",\n",
    "r\"\\bdon't\\b\": \"do not\",\n",
    "r\"\\bdoesn't\\b\": \"does not\",\n",
    "r\"\\bdidn't\\b\": \"did not\",\n",
    "r\"\\bhasn't\\b\": \"has not\",\n",
    "r\"\\bhaven't\\b\": \"have not\",\n",
    "r\"\\bhadn't\\b\": \"had not\",\n",
    "r\"\\bwon't\\b\": \"will not\",\n",
    "r\"\\bwouldn't\\b\": \"would not\",\n",
    "r\"\\bcan't\\b\": \"can not\",\n",
    "r\"\\bcannot\\b\": \"can not\",\n",
    "}\n",
    "\n",
    "def create_ngram_model(params=None):\n",
    "    def preprocessor(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        for k in emo_repl_order:\n",
    "            tweet = tweet.replace(k, emo_repl[k])\n",
    "        for r, repl in re_repl.items():\n",
    "            tweet = re.sub(r, repl, tweet)\n",
    "        return tweet\n",
    "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor,\n",
    "analyzer=\"word\")\n",
    " # ...\n",
    "``` \n",
    " \n",
    "더 많은 단축어가 있지만 이 제한된 집합을 가지고 감성 대 비감성 판별은  70.7 %까지 향상됨\n",
    "\n",
    "```\n",
    "== Pos vs. neg ==\n",
    "0.808   0.024   0.885   0.029\n",
    "== Pos/neg vs. irrelevant/neutral ==\n",
    "0.793   0.010   0.685   0.024\n",
    "== Pos vs. rest ==\n",
    "0.890   0.011   0.517   0.041\n",
    "== Neg vs. rest ==\n",
    "0.886   0.006   0.624   0.033\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어의 종류 고려\n",
    "\n",
    "### 단어의 종류 판단\n",
    "\n",
    "품사(POS, part of speech)를 붙임으로써 단어의 종류를 파악. POS tagger는 전체 문장을 dependence tree(의존 트리)로 만들기 위해 문장을 파싱. 의존 트리의 각 노드는 단어와 의존성을 결정하는 parent-child relationship 로 대응됨.\n",
    "\n",
    "NLTK에는 모든 종류의 파서와 태거(nltk.pos_tag())가 포함되어 있음. 단어 token의 리스트를 입력하면, 각 원소의 원 문자으이 단어와 품사로 된 튜플 리스트가 반환됨.\n",
    "\n",
    "```\n",
    ">>> import nltk\n",
    ">>> nltk.pos_tag(nltk.word_tokenize(\"This is a good book.\"))\n",
    "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('book',\n",
    "'NN'), ('.', '.')]\n",
    ">>> nltk.pos_tag(nltk.word_tokenize(\"Could you please book the\n",
    "flight?\"))\n",
    "[('Could', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('book', 'NN'),\n",
    "('the', 'DT'), ('flight', 'NN'), ('?', '.')]\n",
    "The POS tag abbreviations are taken from the Penn Treebank (adapted from http://www.anc.org/OANC/penn.html):\n",
    "```\n",
    "\n",
    "이러한 태그를 이용해 pos_tag()의 결과로부터 원하는 품사를 구분하는 일이 쉬워짐. 단순하게 NN의 명사, VB의 동사, JJ의 형용사, RB의 부사로 모든 단어를 세어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet을 이용한 성공적인 편법\n",
    "\n",
    "SentiWordNet (http://sentiwordnet.isti.cnr.it)을 내려받으면, 긍정/부정 값이 부여된 대부분의 영어 단어가 13MB 파일 안에 있음.\n",
    "\n",
    "p175 표 참조\n",
    "\n",
    "아래의 load_sent_word_net()은 key가 word type/word 형태의 문자열인 딕셔너리를 반환한다. 예를 들면 'n/implant'로 긍정 또는 부정적인 점수임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import csv, collections\n",
    "def load_sent_word_net():\n",
    "   # making our life easier by using a dictionary that automatically creates an empty list whenever we access a not yet existing key\n",
    "   sent_scores = collections.defaultdict(list)\n",
    "   \n",
    "   with open(os.path.join(DATA_DIR, SentiWordNet_3.0.0_20130122.txt\"),\n",
    "\"r\") as csvfile:\n",
    "\n",
    "      reader = csv.reader(csvfile, delimiter='\\t',\n",
    "quotechar='\"')\n",
    "\n",
    "      for line in reader:\n",
    "         if line[0].startswith(\"#\"):\n",
    "            continue\n",
    "         if len(line)==1:\n",
    "continue\n",
    "         POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n",
    "         if len(POS)==0 or len(ID)==0:\n",
    "            continue\n",
    "            \n",
    "         for term in SynsetTerms.split(\" \"):\n",
    "            # drop number at the end of every term\n",
    "            term = term.split(\"#\")[0]\n",
    "            term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "            key = \"%s/%s\"%(POS, term.split(\"#\")[0])\n",
    "            sent_scores[key].append((float(PosScore), float(NegScore)))\n",
    "    for key, value in sent_scores.items():\n",
    "        sent_scores[key] = np.mean(value, axis=0)\n",
    "    return sent_scores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫번째 estimator\n",
    "\n",
    "모든 기능을 다 넣어 하나의 첫번째 vectorizer를 만들어 보자. 다음 3개의 method를 구현하여 BaseEstimator로 상속받아 처리한다.\n",
    "\n",
    "- get_feature_names(): transform()에서 사용될 속성 이름 리스트를 반환\n",
    "\n",
    "- fit(document, y=None): 분류기를 구현하지 않기 때문에 이 method를 무시하고 self를 반환\n",
    "\n",
    "- transform(documents): shape (len(documents), len(get_feature_names))의 배열을 포함한 numpy.array()를 return한다. documents 안의 모든 document에 대해 get_feature_names()에 있는 모든 속성 값을 반환해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sent_word_net = load_sent_word_net()\n",
    "\n",
    "class LinguisticVectorizer(BaseEstimator):\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['sent_neut', 'sent_pos', 'sent_neg',\n",
    "         'nouns', 'adjectives', 'verbs', 'adverbs',\n",
    "         'allcaps', 'exclamation', 'question', 'hashtag',\n",
    "         'mentioning'])\n",
    "         \n",
    "    # we don't fit here but need to return the reference\n",
    "    # so that it can be used like fit(d).transform(d)\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "        \n",
    "    def _get_sentiments(self, d):\n",
    "    \n",
    "        sent = tuple(d.split())\n",
    "        tagged = nltk.pos_tag(sent)\n",
    "        \n",
    "        pos_vals = []\n",
    "        neg_vals = []\n",
    "        \n",
    "        nouns = 0.\n",
    "        adjectives = 0.\n",
    "        verbs = 0.\n",
    "        adverbs = 0.\n",
    "        \n",
    "        for w,t in tagged:\n",
    "            p, n = 0,0\n",
    "            sent_pos_type = None\n",
    "            if t.startswith(\"NN\"):\n",
    "                sent_pos_type = \"n\"\n",
    "                nouns += 1\n",
    "            elif t.startswith(\"JJ\"):\n",
    "                sent_pos_type = \"a\"\n",
    "                adjectives += 1\n",
    "                elif t.startswith(\"VB\"):\n",
    "                sent_pos_type = \"v\"\n",
    "                verbs += 1\n",
    "            elif t.startswith(\"RB\"):\n",
    "                sent_pos_type = \"r\"\n",
    "                adverbs += 1\n",
    "                \n",
    "            if sent_pos_type is not None:\n",
    "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
    "                \n",
    "                if sent_word in sent_word_net:\n",
    "                    p,n = sent_word_net[sent_word]\n",
    "            pos_vals.append(p)\n",
    "            neg_vals.append(n)\n",
    "            \n",
    "        l = len(sent)\n",
    "        avg_pos_val = np.mean(pos_vals)\n",
    "        avg_neg_val = np.mean(neg_vals)\n",
    "        return [1-avg_pos_val-avg_neg_val, avg_pos_val, avg_neg_val, nouns/l, adjectives/l, verbs/l, adverbs/l]\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs = np.array([self._get_sentiments(d) for d in documents]).T\n",
    "        \n",
    "        allcaps = []\n",
    "        exclamation = []\n",
    "        question = []\n",
    "        hashtag = []\n",
    "        mentioning = []\n",
    "        \n",
    "        for d in documents:\n",
    "            allcaps.append(np.sum([t.isupper() \\\n",
    "               for t in d.split() if len(t)>2]))\n",
    "            \n",
    "            exclamation.append(d.count(\"!\"))\n",
    "            question.append(d.count(\"?\"))\n",
    "            hashtag.append(d.count(\"#\"))\n",
    "            mentioning.append(d.count(\"@\"))\n",
    "            \n",
    "       result = np.array([obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs, allcaps, exclamation, question, hashtag, mentioning]).T\n",
    "       \n",
    "        return result\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두 합치기\n",
    "\n",
    "언어적인 특징과 TfidfVectorizer를 통합해야 하며, scikit-learn의 FeatureUnion 클래스를 사용하면 됨. Pipeline과 같은 방법으로 초기화 되지만, 대신 이전의 출력이 다음 입력으로 전달되어 마지막으로 estimator가 평가하는 Pipeline과 달리 FeatureUnion은 parallel하게 실행하고 결과 벡터를 모두 합친다.\n",
    "\n",
    "```\n",
    "def create_union_model(params=None):\n",
    "    def preprocessor(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        for k in emo_repl_order:\n",
    "            tweet = tweet.replace(k, emo_repl[k])\n",
    "        for r, repl in re_repl.items():\n",
    "            tweet = re.sub(r, repl, tweet)\n",
    "        return tweet.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor, analyzer=\"word\")\n",
    "    ling_stats = LinguisticVectorizer()\n",
    "    all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
    "    clf = MultinomialNB()\n",
    "    pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
    "    if params:\n",
    "        pipeline.set_params(**params)\n",
    "    return pipeline\n",
    "```\n",
    "\n",
    "Training and testing on the combined featurizers, gives another 0.4 percent improvement on average P/R AUC for positive versus negative:\n",
    "```\n",
    "== Pos vs. neg ==\n",
    "0.810   0.023   0.890   0.025\n",
    "== Pos/neg vs. irrelevant/neutral ==\n",
    " 0.791   0.007   0.691\n",
    "== Pos vs. rest ==\n",
    "0.890   0.011   0.529\n",
    "== Neg vs. rest ==\n",
    "0.883   0.007   0.617\n",
    "time spent: 214.12578797340393\n",
    "```\n",
    "\n",
    "이 결과로 긍정 vs 나머지, 부정 vs 나머지에 대한 분류기를 사용하지 않고, 트윗에 감정 요소가 포함되어 있는지 결정하는 첫번째 분류기를 사용하고, 그 후 실제 감성을 결정하는데 긍정적 대 부정적 분류기를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "지금까지 Naïve Bayes가 어떻게 작동하고 전혀 naïve 하지 않은 것을 배움.\n",
    "\n",
    "특히 training set에서 class probability space에 모든 niche를 배울만큼 충분한 데이터가 없을때, Naïve Bayes 는 generalizing에 좋은 성능을 보이지 못함.\n",
    "\n",
    "다음 우리는 tweet에 어떻게 적용하고 rough tweets' texts를 cleaning하는 것이 많은 도움이 된다는 것을 배웠음.\n",
    "\n",
    "마지막으로  약간의  \"cheating\"이 괜찮다는 것을 깨달았고, SentiWordNet을 사용함으로써 분류기의 성능 향상을 가져왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
